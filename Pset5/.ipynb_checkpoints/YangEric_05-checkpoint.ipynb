{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# homework 05: a mixture of five\n",
    "* Eric Yang\n",
    "* 10/11/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.random as rand\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "import scipy.special as special\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in file, adopted from w05-visualize.py\n",
    "def read_data(infile):\n",
    "    '''\n",
    "    read_data(infile)\n",
    "    Read Lestrade's input file, w05-data.tbl, or a file in that format.\n",
    "    Return:\n",
    "       ctype[0..N-1] : cell types 0..Q-1 for each cell i\n",
    "       data[i,g]     : array of count data; rows = cells i; cols = genes g\n",
    "       N             : number of cells (rows of the data file)\n",
    "       G             : number of genes (cols of the data file, after the first)\n",
    "       Q             : number of cell types\n",
    "    '''\n",
    "    ctype = []\n",
    "    data  = []\n",
    "    with open(infile) as f:\n",
    "        for line in f:\n",
    "            if line[0] == '#': continue   # skip comment lines\n",
    "            fields = line.split()\n",
    "            ctype.append(int(fields[1]))\n",
    "            data.append( [int(fields[2]), int(fields[3])])  # assumes exactly 2 genes!!\n",
    "    ctype = np.array(ctype)\n",
    "    data  = np.array(data)\n",
    "    N, G  = np.shape(data)\n",
    "    Q     = np.max(ctype) + 1\n",
    "    return ctype, data, N, G, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in file\n",
    "ctype, data, N, G, Q = read_data('w05-data.tbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. reproduce Wiggins' K-means result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(points, k, n_runs):\n",
    "    '''\n",
    "    performs k-means clustering on given 2D datapoints (caraway, kiwi)\n",
    "    k specifies number of clusters \n",
    "    n_runs specifies number of runs\n",
    "    result is a cluster plot that shows the best result based on minimum total distance from all points to assigned centroids\n",
    "    '''\n",
    "    # initialize best clusters\n",
    "    min_dist = float('inf')\n",
    "    best_clusters = []\n",
    "    \n",
    "    # split points into caraways and kiwis\n",
    "    caraway = [i[0] for i in data]\n",
    "    kiwi = [i[1] for i in data]\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        # initialize random set of centers based on max and min of 2D points\n",
    "        centroids = []\n",
    "        for i in range(k):\n",
    "            centroids.append([np.random.choice(np.linspace(min(caraway), max(caraway), 100)),\n",
    "                            np.random.choice(np.linspace(min(kiwi), max(kiwi), 100))]) \n",
    "        \n",
    "        # initialize clusters\n",
    "        clusters_prev = []*len(points)\n",
    "        clusters = [0]*len(points)\n",
    "        \n",
    "        # stop when clusters don't change\n",
    "        while clusters_prev != clusters:\n",
    "            clusters_prev = clusters\n",
    "            # assign clusters\n",
    "            clusters,distance = assign_clusters(points, centroids)\n",
    "            # update centroids\n",
    "            centroids = update_centroids(points, clusters, k)\n",
    "            \n",
    "        # save best clusters, centers and min_dist\n",
    "        if distance < min_dist:\n",
    "            best_clusters = clusters\n",
    "            best_centroids = centroids\n",
    "            min_dist = distance\n",
    "\n",
    "    # plot data\n",
    "    plot_clusters(points, best_clusters, best_centroids)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_clusters(points, centroids):\n",
    "    '''\n",
    "    assigns each point to the closest centroid, distance defined by euclidean distance\n",
    "    returns assigned cluster for each point and total squared distance\n",
    "    '''\n",
    "    # assign clusters\n",
    "    clusters = [] # int list of length len(points)\n",
    "    distances = [] # float list of length len(points)\n",
    "    for point in points:\n",
    "        dist_to_centroids = [] \n",
    "        for centroid in centroids:\n",
    "            dist = math.sqrt((point[0]-centroid[0])**2 + (point[1]-centroid[1])**2)\n",
    "            dist_to_centroids.append(dist)\n",
    "        clusters.append(np.argmin(dist_to_centroids))\n",
    "        distances.append(np.min(dist_to_centroids))\n",
    "    \n",
    "    # dictionary mapping cluster to number of points in that cluster\n",
    "    counts = Counter(clusters)\n",
    "    \n",
    "    # check for empty clusters, assign those with assign_empty helper function\n",
    "    for cluster in range(len(centroids)):\n",
    "        if cluster not in counts.keys():\n",
    "            clusters, distances = assign_empty(cluster, clusters, distances)\n",
    "\n",
    "    return clusters, sum(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_empty(empty_cluster, clusters, distances):\n",
    "    '''\n",
    "    assigns empty clusters to a point that is farthest away from any cluster\n",
    "    updates clusters and distances lists\n",
    "    '''\n",
    "    # find point farthest away from any cluster\n",
    "    idx = np.argmax(distances)\n",
    "    \n",
    "    # assign empty cluster to that point\n",
    "    clusters[idx] = empty_cluster\n",
    "    \n",
    "    # set distance for that point to 0 since it is on a cluster\n",
    "    distances[idx] = 0\n",
    "    \n",
    "    return clusters, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_centroids(points, clusters, k):\n",
    "    '''\n",
    "    updates centroids as mean of all the points assigned to that cluster\n",
    "    k is number of clusters\n",
    "    '''\n",
    "    points = np.asarray(points)\n",
    "    clusters = np.asarray(clusters)\n",
    "    \n",
    "    # get indexes of each point assigned to each cluster\n",
    "    cluster_indexes = [] # each item inside the list is a list of indexes\n",
    "    for cluster in range(k):\n",
    "        cluster_indexes.append(np.where(clusters == cluster))\n",
    "    \n",
    "    # find new centroid for each cluster by average (caraway, kiwi)\n",
    "    centroids = []\n",
    "    for cluster in range(k):\n",
    "        centroids.append(np.mean(points[cluster_indexes[cluster]],axis=0))\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from w05-visualize.py\n",
    "def plot_clusters(points, clusters, centroids):\n",
    "    \n",
    "    N, G  = np.shape(data)\n",
    "    Q, G2 = np.shape(centroids)\n",
    "    centroids = np.asarray(centroids)\n",
    "    \n",
    "    # assign colors\n",
    "    colormap = ['xkcd:mustard', 'xkcd:rose','xkcd:olive','xkcd:azure','xkcd:orange']\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    for i in range(N):\n",
    "        edgecolor = colormap[clusters[i]]\n",
    "        fillcolor = 'w'\n",
    "        shape     = 'o'\n",
    "        ax.loglog(points[i,0], points[i,1], marker=shape, mec=edgecolor, mfc=fillcolor, mew=1.5)\n",
    "\n",
    "    for q in range(Q):\n",
    "        ax.loglog(centroids[q,0], centroids[q,1], '*k', ms=10)\n",
    "\n",
    "    ax.set_xlabel('caraway (counts)')\n",
    "    ax.set_ylabel('kiwi (counts)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-32f3a7cb73b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mk_means\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-d1326013383a>\u001b[0m in \u001b[0;36mk_means\u001b[0;34m(points, k, n_runs)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mclusters_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# assign clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mclusters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcentroids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;31m# update centroids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mcentroids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_centroids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-3eebdaff7d32>\u001b[0m in \u001b[0;36massign_clusters\u001b[0;34m(points, centroids)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcentroid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcentroid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mdist_to_centroids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mclusters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_to_centroids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mdistances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_to_centroids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmin\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \"\"\"\n\u001b[0;32m-> 1101\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k_means(data, 5, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is K-means clustering producing this result, when there are clearly five distinct clusters in the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There does not seem to be anything wrong with the k-means implementation. It seems like the issue is that the 5 clusters exist in logspace, while the cluster assignments in k-means are done in standard euclidean space. I will explore a solution towards this in part 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. mixture negative binomial fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partially adapted from w05-section_jupyter_notebook.ipynb\n",
    "def negative_binomial_em(points, k, n_runs, nll_thresh):\n",
    "    '''\n",
    "    performs expectation maximization with negative binomial to cluster cells given 2D gene datapoints (caraway, kiwi)\n",
    "    k specifies number of clusters \n",
    "    n_runs specifies number of runs\n",
    "    nll_thresh determines when to stop clustering iteration for each run\n",
    "    result is a cluster plot that shows the best result based on minimum negative log likelihood of data given mixture models\n",
    "    returns means of all clusters, and number of datapoints in each cluster\n",
    "    '''\n",
    "    \n",
    "    # define best mus (mus_best), best distance to mu (min_dist), best mixture coefficients (mix_coeffs_best\n",
    "    # and intialize as starting best nll (nll_best) \n",
    "    min_dist = float('inf')\n",
    "    mus_best = []*len(points)\n",
    "    nll_best = float('inf')\n",
    "    mix_coeffs_best = []\n",
    "    \n",
    "    # split points into caraways and kiwis\n",
    "    caraway = [i[0] for i in data]\n",
    "    kiwi = [i[1] for i in data]\n",
    "    \n",
    "    for i in range(n_runs):\n",
    "        # Each iteration starts with a random set of mus (mus) with random mixture coeffcients (mix_coeffs)\n",
    "        mix_coeffs = rand.rand(k)\n",
    "        mix_coeffs = np.divide(mix_coeffs,sum(mix_coeffs))\n",
    "        mus = []\n",
    "        for i in range(k):\n",
    "            mus.append([np.random.choice(np.linspace(min(caraway), max(caraway), 100)),\n",
    "                            np.random.choice(np.linspace(min(kiwi), max(kiwi), 100))]) \n",
    "            \n",
    "        # We will iterate until the nll_diff <= (nll_thresh* nll_old)\n",
    "        # Here we initialize holder variables for the last nll and the difference between the current and last nll\n",
    "        nll_diff = float('inf')\n",
    "        nll_old = float(0)    \n",
    "        \n",
    "        # Iterate while the differnece between consecutive nlls is above a threshold\n",
    "        while nll_diff > (nll_thresh* nll_old):\n",
    "        \n",
    "            # Calculate posterior probabilities and assign points to clusters\n",
    "            posts,clusters = expectation(points, mus, 0.3, mix_coeffs) # phi = 0.3\n",
    "            \n",
    "            # Calculate new mus and mixture coefficients given current posterior probabilities\n",
    "            mus, mix_coeffs = maximization(points, posts, mix_coeffs)    \n",
    "            \n",
    "            # Calcualte the nll of the current mixture model\n",
    "            nll = negll(points, mus, 0.3, mix_coeffs) # phi = 0.3\n",
    "            \n",
    "            # find difference in consecutive nlls and update the nll_old\n",
    "            nll_diff = abs(nll-nll_old)\n",
    "            nll_old = nll\n",
    "            \n",
    "        # Update best estimates for mus, mixture coefficients and cluster assignments\n",
    "        if nll < nll_best:\n",
    "            mix_coeffs_best = mix_coeffs\n",
    "            nll_best = nll\n",
    "            mus_best = mus\n",
    "            clusters_best = clusters\n",
    "        \n",
    "    # plot data\n",
    "    plot_clusters(points, clusters_best, mus_best)    \n",
    "    \n",
    "    return mus_best, Counter(clusters_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partially adapted from w05-section_jupyter_notebook.ipynb\n",
    "def expectation(points, mus, phi, pis):\n",
    "    '''\n",
    "    calculates posterior prob of point belonging to each component\n",
    "    assigns each point to a cluster based on max posterior prob\n",
    "    \n",
    "    Inputs:\n",
    "    points        : list or 1D array, (caraway,kiwi) of all points\n",
    "    mus           : list or 1D array, current estimates for component means\n",
    "    phi           : float, set parameter for component distribution\n",
    "    pis           : list or 1D array, current extiamtes for mixture coefficients of components\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    posts    : 2D array, Posterior probabilities of each component being the source of each point.\n",
    "    clusters : list or 1D array, list of current cluster assignment for each point \n",
    "    '''\n",
    "    \n",
    "    # initialize P(q|x)\n",
    "    Pqx = [] # each row is a point, each column is a component\n",
    "    \n",
    "    # scipy.stats.nbinom.pmf takes in n, p\n",
    "    n = 1/phi\n",
    "    p = 1/(1+np.array(mus)*phi)\n",
    "    \n",
    "    # get posterior probs with negative binomial distribution for each component of each point\n",
    "    for point in points:\n",
    "        point_prob = []\n",
    "        for idx, mu in enumerate(mus):\n",
    "            prob = stats.nbinom.pmf(point[0],n,p[idx,0]) * stats.nbinom.pmf(point[1],n,p[idx,1])\n",
    "            point_prob.append(prob)\n",
    "        Pqx.append(point_prob)\n",
    "    \n",
    "    # normalize so that probs of each point sum to 1\n",
    "    posts = []\n",
    "    for point in Pqx:\n",
    "        total_prob = sum(point)\n",
    "        post = [x / total_prob for x in point]\n",
    "        posts.append(post)\n",
    "    \n",
    "    # assign each point to cluster\n",
    "    clusters = []\n",
    "    for point in posts:\n",
    "        clusters.append(np.argmax(point))\n",
    "        \n",
    "    return posts, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partially adapted from w05-section_jupyter_notebook.ipynb\n",
    "def maximization(points, posts, pis):\n",
    "    '''\n",
    "    Return the updated mu and pi for each component\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    points        : list or 1D array, (caraway,kiwi) of all points\n",
    "    posts         : 2D array, Posterior probabilities of each component being the source of each point\n",
    "    pis           : list or 1D array, list of current extiamtes for mixture coefficients of components\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mus           : list or 1D array, current estimate of component means\n",
    "    new_pis       : list or 1D array, updated list of extimates for mixture coefficients of components \n",
    "    '''\n",
    "    \n",
    "    # convert to np array for mathematical manipulation of 2D lists\n",
    "    points = np.array(points)\n",
    "    posts = np.array(posts)\n",
    "    \n",
    "    # calculate the new mean for each component q as the posterior weighted average of point positions \n",
    "    # each column represents a component in our matrix of posterior probabilities (posts)\n",
    "    mus = []\n",
    "    denominators = []\n",
    "    for k in range(len(pis)):    \n",
    "        numerator = 0\n",
    "        denominator = 0\n",
    "        for idx, point in enumerate(points):\n",
    "            numerator += np.array(point) * posts[idx, k]\n",
    "            denominator += posts[idx, k]\n",
    "        mus.append((numerator/denominator).tolist())\n",
    "        denominators.append(denominator)\n",
    "    \n",
    "    # calculate the new mixture coefficients as the mean of the posteriors\n",
    "    # the updated mixture coefficients π are the expected fraction of data points assigned to each component \n",
    "    new_pis = np.array(denominators)/len(points)\n",
    "    \n",
    "    return mus, new_pis.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partiallly adapted from w05-section_jupyter_notebook.ipynb\n",
    "def negll(points, mus, phi, pis):\n",
    "    '''\n",
    "    Return the negative log likelihood of data given the current mixture model\n",
    "    \n",
    "    Inputs:\n",
    "    points        : list or 1D array, (caraway,kiwi) of all points\n",
    "    mus           : list or 1D array, current estimates for component means\n",
    "    phi           : float, set parameter for component distribution\n",
    "    pis           : list or 1D array, current extiamtes for mixture coefficients of components\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    nll       : float, negative log likelihood of data given model\n",
    "    '''\n",
    "    # scipy.stats.nbinom.pmf takes in n, p\n",
    "    n = 1/phi\n",
    "    p = 1/(1+np.array(mus)*phi)\n",
    "    \n",
    "    # Compute the nll as the normal logpdf of the data, given mu and sigma, plus the log of the mixture coefficient\n",
    "    # Summed for each point\n",
    "    denominators = []\n",
    "    for point in points:\n",
    "        prob = []\n",
    "        for idx, mu in enumerate(mus):\n",
    "            prob.append(np.log10(pis[idx]) + stats.nbinom.logpmf(point[0],n,p[idx,0]) + stats.nbinom.logpmf(point[1],n,p[idx,1]))\n",
    "        denominators.append(special.logsumexp(prob))\n",
    "    \n",
    "    return -sum(denominators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means, counts = negative_binomial_em(data, 5, n_runs=10, nll_thresh=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note that these results may be slightly different each run of the notebook due to random initial assignment\n",
    "The estimated mean expression levels of (Caraway, Kiwi) with count as units are:\n",
    "1. (1920.2, 1865.6)\n",
    "2. (31.3, 31.5)\n",
    "3. (31.4, 1996.0)\n",
    "4. (310.4, 305.6)\n",
    "5. (2055.5, 29.3)\n",
    "\n",
    "For each of the above clusters, the relative proportions of each cell type in the 1000 cells are:\n",
    "1. 0.204\n",
    "2. 0.22\n",
    "3. 0.106\n",
    "4. 0.373\n",
    "5. 0.097"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. find a simple fix for K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted above, the clusters exist in logspace while the cluster assignments are done in standard euclidean space. There are several ways to address this, one of which involves simply initializes the centroids randomly in logspace, allowing initial centroids to skew more towards the lower counts. This allows standard euclidean distance calculations to reveal clusters in the logspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_fix(points, k, n_runs):\n",
    "    '''\n",
    "    performs k-means clustering on given 2D datapoints (caraway, kiwi), addresses clusters that exist in logspace\n",
    "    k specifies number of clusters \n",
    "    n_runs specifies number of runs\n",
    "    result is a cluster plot that shows the best result based on minimum total distance from all points to assigned centroids\n",
    "    '''\n",
    "    # initialize best clusters\n",
    "    min_dist = float('inf')\n",
    "    best_clusters = []\n",
    "    \n",
    "    # split points into caraways and kiwis\n",
    "    caraway = [i[0] for i in data]\n",
    "    kiwi = [i[1] for i in data]\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        # initialize random set of centers based on max and min of 2D points in logspace\n",
    "        centroids = []\n",
    "        for i in range(k):\n",
    "            centroids.append([np.random.choice(np.logspace(np.log10(min(caraway)), np.log10(max(caraway)))),\n",
    "                            np.random.choice(np.logspace(np.log10(min(kiwi)), np.log10(max(kiwi))))])\n",
    "        # initialize clusters\n",
    "        clusters_prev = []*len(points)\n",
    "        clusters = [0]*len(points)\n",
    "        \n",
    "        # stop when clusters don't change\n",
    "        while clusters_prev != clusters:\n",
    "            clusters_prev = clusters\n",
    "            # assign clusters\n",
    "            clusters,distance = assign_clusters(points, centroids)\n",
    "            # update centroids\n",
    "            centroids = update_centroids(points, clusters, k)\n",
    "            \n",
    "        # save best clusters, centers and min_dist\n",
    "        if distance < min_dist:\n",
    "            best_clusters = clusters\n",
    "            best_centroids = centroids\n",
    "            min_dist = distance\n",
    "\n",
    "    # plot data\n",
    "    plot_clusters(points, best_clusters, best_centroids)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k_means_fix(data, 5, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
